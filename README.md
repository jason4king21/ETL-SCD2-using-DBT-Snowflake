# ğŸ“Š ETL with Slowly Changing Dimension Type 1 (SCD1) using Snowflake Tasks, Streams & Stored Procedure

This project demonstrates an automated ETL pipeline that loads `.csv` files into Snowflake from an Amazon S3 bucket, and processes updates using **SCD Type 1** logic via **Snowflake Tasks, Streams, and Stored Procedures**.

---

## ğŸ§© Architecture Overview
![Real-Time Streaming Pipeline](diagrams/architecture.png)

1. **Anaconda Python Environment**  
   - Reads a `.csv` file from local storage  
   - Uploads it to S3 using `boto3`  
   - Uses a `.yml` config file to securely manage AWS Secrets via Anaconda

2. **Amazon S3**  
   - Serves as the landing zone for raw CSV files

3. **Snowflake Configuration**
   - **External Stage**: Points to the S3 bucket
   - **Stream**: Tracks new rows in a staging table
   - **Task**: Executes every minute to check for new files
   - **Stored Procedure**: Implements SCD Type 1 merge logic (insert or update)

---

## ğŸ”§ Tech Stack

- **Python (Anaconda)** â€“ For uploading files and managing AWS credentials
- **Amazon S3** â€“ Storage layer for raw data
- **Snowflake**  
  - **Stages** â€“ Reference S3 objects  
  - **Streams** â€“ Change data capture  
  - **Tasks** â€“ Scheduled execution  
  - **Stored Procedures** â€“ SCD1 logic in SQL/JavaScript

---

## ğŸ“‚ Repository Structure
```
ETL-SCD1-Snowflake-Tasks-Streams-StoredProcedures/
â”‚
â”œâ”€â”€ README.md                          â†’ Project overview and setup instructions
â”œâ”€â”€ .gitignore                         â†’ Specifies files and directories to ignore in version control
â”œâ”€â”€ diagrams/
â”‚   â”œâ”€â”€ architecture.png               â†’ Visual representation of the pipeline flow
â”‚   â””â”€â”€ Diagram Generator/
â”‚       â””â”€â”€ architecture.py            â†’ Python script to generate the architecture diagram
â”œâ”€â”€ python/
â”‚   â””â”€â”€ UploadFileToS3.py              â†’ Python script to upload CSV file to Amazon S3
â”œâ”€â”€ snowflake/
â”‚   â”œâ”€â”€ SnowflakeSetup.sql             â†’ Creates external stage in Snowflake
â”‚   â”œâ”€â”€ SnowpipeSetup.sql              â†’ Defines Snowpipe for auto-ingestion from S3
â”‚   â”œâ”€â”€ StreamSetup.sql                â†’ Defines stream on staging table for change tracking
â”‚   â””â”€â”€ TaskAndStoredProcSetup.sql     â†’ Creates Snowflake task and stored procedure for SCD1 merge
â”œâ”€â”€ tests/
    â””â”€â”€ [test files]                   â†’ Placeholder directory for unit tests or validation scripts
```

---

## â–¶ï¸ How It Works

1. ğŸ Run the Python script to upload `customer_full_data.csv` to S3:
   ```bash
   python UploadFileToS3.py

2. ğŸ—ï¸ In Snowflake:
    - Create an external stage to the S3 bucket
    - Create a stream to track staging table changes
    - Create a task to run every minute
    - Task calls a Stored Procedure to perform SCD Type 1 updates

3. ğŸ§  The Stored Procedure logic:
    - Inserts new rows
    - Updates existing records based on a business key (e.g., customer_id)

---

## âœ… Highlights

- Real-world use of Snowflake Tasks, Streams, and Stored Procedures
- Secure AWS credential management using .yml
- SCD1 logic for maintaining clean and updated dimension tables

---

## ğŸ·ï¸ Tags & Topics
```
Use these hashtags when sharing the project:
#DataEngineering #Snowflake #SCD1 #ETL #Python #AWS #S3 #StoredProcedure #Tasks #Streams #DataPipeline #Anaconda #CloudData

```
