# ğŸ§± ETL with Slowly Changing Dimension Type 2 (SCD2) using DBT & Snowflake

This project showcases a modular ETL pipeline that loads `.csv` files from AWS S3 into Snowflake and applies **SCD Type 2** logic using **DBT models and Snapshots**, capturing historical changes across dimension tables.

---

## ğŸ§© Architecture Overview
![SCD2 Pipeline Diagram](diagrams/architecture.png)

1. **Raw Data Upload**  
   - Files are dropped into an S3 bucket  
   - Managed using a Python script or automated service

2. **AWS PrivateLink**  
   - Securely routes S3 traffic into Snowflake

3. **Snowflake Configuration**  
   - **External Stage**: Reads files from S3  
   - **Bronze Layer**: Raw data loaded via `COPY INTO`  
   - **Silver Layer**: Cleaned and modeled data  
   - **Snapshots**: Track changes across time using surrogate keys and date ranges  
   - **Gold Layer**: Final reporting views

---

## ğŸ”§ Tech Stack

- **Python** â€“ Upload `.csv` files to S3
- **Amazon S3** â€“ Stores raw input files
- **Snowflake**  
  - **External Stage** â€“ Reads files from S3  
  - **Warehouse** â€“ Performs transformations and queries
- **DBT (Data Build Tool)**  
  - **Models** â€“ Define Bronze, Silver, and Gold layers  
  - **Snapshots** â€“ Implement SCD Type 2 tracking logic  
  - **Tests** â€“ Validate assumptions and data integrity

---

## ğŸ“‚ Repository Structure

```
ETL-SCD2-USING-DBT-SNOWFLAKE/
â”‚
â”œâ”€â”€ analyses/ â†’ (Optional) Directory for ad hoc SQL analysis or documentation
â”œâ”€â”€ diagrams/
â”‚ â”œâ”€â”€ Diagram Generator/
â”‚ â”‚ â””â”€â”€ architecture.py â†’ Python script to generate the architecture diagram
â”‚ â””â”€â”€ architecture.png â†’ Visual representation of the data pipeline
â”‚
â”œâ”€â”€ macros/
â”‚ â”œâ”€â”€ copy_into_snowflake.sql â†’ Custom macro to load data into Snowflake tables
â”‚ â”œâ”€â”€ generate_schema_name.sql â†’ Dynamically sets schema names based on environment
â”‚ â””â”€â”€ query_tag.sql â†’ Tags queries for better observability and debugging
â”‚
â”œâ”€â”€ models/
â”‚ â”œâ”€â”€ gold/
â”‚ â”‚ â””â”€â”€ product_view.sql â†’ Final business layer with cleaned, historical data
â”‚ â””â”€â”€ silver/
â”‚ â”œâ”€â”€ schema.yml â†’ Describes metadata and tests for silver models
â”‚ â””â”€â”€ transform_product_load.sql â†’ Applies SCD2 transformations to raw data
â”‚
â”œâ”€â”€ python/
â”‚ â””â”€â”€ local_to_aws_s3.py â†’ Python script to upload data from local to AWS S3
â”‚
â”œâ”€â”€ seeds/
â”‚ â””â”€â”€ Product_Dim.csv â†’ Seed file used to populate static/starter dimension data
â”‚
â”œâ”€â”€ snapshots/
â”‚ â””â”€â”€ product_snapshot.sql â†’ Snapshot definition for SCD2 logic in DBT
â”‚
â”œâ”€â”€ snowflake/
â”‚ â””â”€â”€ SnowflakeSetup.sql â†’ Creates stage, tables, and roles in Snowflake
â”‚
â”œâ”€â”€ target/ â†’ DBT-generated compiled code and artifacts (ignored from version control)
â”‚
â”œâ”€â”€ tests/
â”‚ â”œâ”€â”€ Product_Dim_1.csv â†’ Test input file for verifying snapshot logic
â”‚ â””â”€â”€ Product_Dim.csv â†’ Another variant of test dimension data
â”‚
â”œâ”€â”€ .gitignore â†’ Files and folders to exclude from git tracking
â”œâ”€â”€ dbt_project.yml â†’ Core DBT project configuration file
â”œâ”€â”€ package-lock.yml â†’ Lockfile for consistent package installations
â”œâ”€â”€ packages.yml â†’ DBT package dependencies
â””â”€â”€ README.md â†’ Project documentation and setup instructions
```

---

## â–¶ï¸ How It Works

1. â˜ï¸ Upload `Product_Dim.csv` to the S3 bucket  
   (optionally via Python, UI, or automation)

2. â„ï¸ In Snowflake:
    - Create a stage and load the file to the Bronze table using `COPY INTO`

3. ğŸ§± With DBT:
    - Define transformation models across Bronze, Silver, and Gold layers
    - Use `dbt snapshot` to track historical changes in the Silver layer

4. ğŸ” DBT Snapshot Logic:
    - Creates new records when non-key fields change
    - Maintains history with `valid_from`, `valid_to`, and `is_current` flags

---

## âœ… Highlights

- Modular, layered architecture (Bronze â†’ Silver â†’ Gold)
- Real-world implementation of DBT Snapshots for SCD2
- Secure ingestion from AWS S3 using Snowflake stages
- Analytical reporting support via clean Gold layer views

---

## ğŸ·ï¸ Tags & Topics
```
Use these hashtags when sharing the project:
#DataEngineering #Snowflake #SCD2 #ETL #DBT #AWS #S3 #Snapshots #DataModeling #Analytics #DataPipeline #CloudWarehouse

```
